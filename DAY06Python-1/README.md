# DAY 06 – Проект-скрапинг
## Как структурировать код
Сегодня вы создадите небольшой "реальный" проект, который будет собирать данные с веб-страниц. Для этого нужно описать несколько классов, которые будут описывать логику каждого этапа работы с данными, а также описать их взаимодействие. Каждый логический модуль принято писать в своем файле-модуле.

## Оглавление

1. [Глава I](#глава-i) \
    1.1. [Преамбула](#преамбула)
2. [Глава II](#глава-ii) \
    2.1. [Общая инструкция](#общая-инструкция)
3. [Глава III](#глава-iii) \
    3.1. [Цели](#цели)
4. [Глава IV](#глава-iv) \
    4.1. [Задание](#задание)
5. [Глава V](#глава-v) \
    5.1. [Сдача работы и проверка](#сдача-работы-и-проверка)

## Глава I
### Преамбула

Грамотная структура организации кода в проекте - это огромная часть работы разработчика. Важный принцип построения архитектуры проекта - это модульность. Части проекта, которые отвечают за разный функционал, не должны сильно зависеть друг от друга, чтобы изменения одного модуля минимально касались остальных. Для достижения этого принципа принято разделять логику модуля на "внутреннюю" и "внешнюю", т.е. API - протокол взаимодействия с модулем. Внешняя логика должна оставаться постоянной, в то время как внутреннюю можно менять хоть каждый день, в зависимости от требований бизнеса. Сегодня вы напишете проект - веб-скрапер - модуль, который умеет добывать и структурировать данные с веб-страниц.

## Глава II
### Общая инструкция

Методология Школы 21 может быть не похожа на тот образовательный опыт, который с вами случался ранее. Ее отличает высокий уровень автономии: у вас есть задача, вы должны ее выполнить. По большей части вам нужно будет самим добывать знания для ее решения. Второй важный момент – это peer-to-peer обучение. В образовательном процессе нет преподавателей и экспертов, перед которыми вы защищаете свой результат. Вы это делаете перед таким же учащимися, как и вы сами. У них есть чек-лист, который поможет им выполнить приемку вашей работы качественно.

Роль Школы 21 заключается в том, чтобы обеспечить через последовательность заданий и оптимальный уровень поддержки такую траекторию обучения, при которой вы освоите не только hard skills, но и научитесь самообучаться.

* Не доверяйте слухам и предположениям о том, как должно быть оформлено ваше решение. Этот документ является единственным источником, к которому стоит обращаться по большинству вопросов.
* Ваше решение будет оцениваться другими учащимися бассейна.
* Подлежат оцениванию только те файлы, которые вы выложили в GIT.
* В вашей папке не должно быть лишних файлов – только те, что были указаны в задании.
* Есть вопрос? Спросите коллегу справа. Не помогло? Спросите коллегу слева.
* Не забывайте, что у вас есть доступ к интернету и поисковым системам.
* Обсуждение заданий можно вести и в Slack бассейна.
* Будьте внимательные к примерам, указанным в этом документе – они могут иметь важные детали, которые не были оговорены другим способом.
* И да пребудет с вами Сила!

Примечание. В папке src хранятся блокноты с конспектами и заданиями. В некоторых из них используются картинки, и чтобы они отображались, в той же папке лежат папки с ними, которые вам нет необходимости просматривать.  


## Глава III
### Цели

Наша цель - научить структурировать код в питоновских проектах, а также узнать, как устроена веб-страница.

## Глава IV
### Задание

Задача скрапера - добыть данные с веб-страниц. Такой подход нужен только тогда, когда у поставщика данных нет какого-то установленного протокола скачивания данных или доступа к хранилищу данных (или всё это стоит денег, которые не вписываются в наш бюджет =) ). Логику выполнения этой задачи можно разделить на три модуля:

1. Модуль, который скачивает код веб-страницы на компьютер, чтобы обрабатывать данные с нее локально;
2. Модуль, который найдет и структурирует данные в исходном коде веб-страницы;
3. Модуль, который обрабатывает данные в соответствии с бизнес-логикой системы.

Также нам нужна точка входа в наше приложение - то место, где будет собран весь основной функционал, который мы хотим сделать доступным пользователем нашего модуля.

Наш проект небольшой - поэтому нам понадобится всего 4 файла для описания вышеперечисленных модулей. Ваша задача - написать эти 4 файла с кодом, где, соответственно, будет описана логика каждого подмодуля:

1. `download.py` - модуль, ответственный за скачивание веб-страницы. Содержит описание класса Downloader (см. блокнот с заданием).
2. `parse.py` - модуль, в котором описана логика выделения данных из исходного кода веб-страницы. Содержит описание класса Parser (см. блокнот с заданием). Рекомендуется для парсинга веб-страницы использовать библиотеку BeautifulSoup4.
3. `data.py` - модуль, в котором описана любая логика обработки полученных и структурированных файлов (на ваш выбор).
4. `__init__.py` - файл, в котором будет описана функция process, которая будет принимать на вход адрес страницы, которую мы хотим заскрапить, опциональным аргументом путь к файлу, который будет хранить копию страницы на локальном компьютере, и еще одним опциональным аргументом - путь к файлу, куда мы сохраним результаты парсинга страницы. Возвращать эта функция должна результат исполнения какой-то части логики, описанной в data.py.

## Глава V
### Сдача работы и проверка

Вам нужно загрузить в GIT в папку `src` папку с файлами `download.py`, `parse.py`, `data.py`, `__init__.py`, в которых будет описан код скрапера.  


